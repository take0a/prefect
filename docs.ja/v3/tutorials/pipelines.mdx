---
title: データパイプラインを構築する
description: Prefect を使用して、回復力とパフォーマンスに優れたデータ パイプラインを構築する方法を学びます。
---

[クイックスタート](/v3/get-started/quickstart)では、GitHubリポジトリのリストにスターを付与するPrefectフローを作成しました。
[フローのスケジュール設定](/v3/tutorials/schedule)では、リモートインフラストラクチャ上でそのフローの実行をスケジュールする方法を学びました。

このチュートリアルでは、このフローを回復力とパフォーマンスに優れたデータパイプラインに変える方法を学習します。

現実世界は複雑で、Prefectはそうした複雑性に対応できるように設計されています。

- APIリクエストが失敗する可能性があります。
- APIリクエストの実行速度が遅すぎる。
- APIリクエストの実行速度が速すぎて、レート制限が発生する。
- 同じタスクを複数回実行することで、時間とコストが無駄になります。

これらの問題をビジネスロジック自体で解決するのではなく、Prefectの組み込み機能を使用して対処しましょう。

## 失敗した場合は再試行する

最初に実行できる改善は、フローに再試行機能を追加することです。
HTTP リクエストが失敗した場合、諦める前に数回再試行することができます。

```python
from typing import Any

import httpx
from prefect import task


@task(retries=3)
def fetch_stats(github_repo: str) -> dict[str, Any]:
    """Task 1: Fetch the statistics for a GitHub repo"""

    api_response = httpx.get(f"https://api.github.com/repos/{github_repo}")
    api_response.raise_for_status() # Force a retry if not a 2xx status code
    return api_response.json()
```

<Expandable title="full example">
再試行の動作を確認するには、次のコードを実行します:

```python
from typing import Any

import httpx
from prefect import flow, task # Prefect flow and task decorators

@task(retries=3)
def fetch_stats(github_repo: str) -> dict[str, Any]:
    """Task 1: Fetch the statistics for a GitHub repo"""

    api_response = httpx.get(f"https://api.github.com/repos/{github_repo}")
    api_response.raise_for_status() # Force a retry if not a 2xx status code
    return api_response.json()


@task
def get_stars(repo_stats: dict[str, Any]) -> int:
    """Task 2: Get the number of stars from GitHub repo statistics"""

    return repo_stats['stargazers_count']


@flow(log_prints=True)
def show_stars(github_repos: list[str]):
    """Flow: Show the number of stars that GitHub repos have"""

    for repo in github_repos:
        # Call Task 1
        repo_stats = fetch_stats(repo)

        # Call Task 2
        stars = get_stars(repo_stats)

        # Print the result
        print(f"{repo}: {stars} stars")



# Run the flow
if __name__ == "__main__":
    show_stars([
        "PrefectHQ/prefect",
        "pydantic/pydantic",
        "huggingface/transformers"
    ])
```
</Expandable>

## 低速タスクの同時実行

個々のAPIリクエストが遅い場合は、複数のリクエストを同時に実行することで、全体として高速化できます。
タスクで`map`メソッドを呼び出すと、タスクランナーに引数のリストが送信され、同時に実行されます（または、[各引数を個別に`.submit()`](/v3/develop/task-runners#access-results-from-submitted-tasks)を実行することもできます）。

```python
from prefect import flow

@flow(log_prints=True)
def show_stars(github_repos: list[str]) -> None:
    """Flow: Show number of GitHub repo stars"""

    # Task 1: Make HTTP requests concurrently
    stats_futures = fetch_stats.map(github_repos)

    # Task 2: Once each concurrent task completes, get the star counts
    stars = get_stars.map(stats_futures).result()

    # Show the results
    for repo, star_count in zip(github_repos, stars):
        print(f"{repo}: {star_count} stars")
```

<Expandable title="full example">
同時タスクの動作を確認するには、次のコードを実行します:

```python
from typing import Any

import httpx
from prefect import flow, task


@task(retries=3)
def fetch_stats(github_repo: str) -> dict[str, Any]:
    """Task 1: Fetch the statistics for a GitHub repo"""
    return httpx.get(f"https://api.github.com/repos/{github_repo}").json()


@task
def get_stars(repo_stats: dict[str, Any]) -> int:
    """Task 2: Get the number of stars from GitHub repo statistics"""
    return repo_stats["stargazers_count"]


@flow(log_prints=True)
def show_stars(github_repos: list[str]) -> None:
    """Flow: Show number of GitHub repo stars"""

    # Task 1: Make HTTP requests concurrently
    stats_futures = fetch_stats.map(github_repos)

    # Task 2: Once each concurrent task completes, get the star counts
    stars = get_stars.map(stats_futures).result()

    # Show the results
    for repo, star_count in zip(github_repos, stars):
        print(f"{repo}: {star_count} stars")


if __name__ == "__main__":
    # Run the flow
    show_stars(
        [
            "PrefectHQ/prefect",
            "pydantic/pydantic",
            "huggingface/transformers"
        ]
    )

```
</Expandable>

<Tip>
`.map()` によって返された Future のリストに対して `.result()` を呼び出すと、すべてのタスクが完了するまでブロックされます。

詳しくは [`.map()` のドキュメント](/v3/develop/task-runners#mapping-over-iterables) をご覧ください。
</Tip>


## レート制限を回避する

タスクを同時実行する場合、使用しているAPIのレート制限に達する可能性が高くなります。
これを回避するには、Prefectを使用してグローバルな同時実行制限を設定します。

```bash
# GitHub has a rate limit of 60 unauthenticated requests per hour (~0.016 requests per second)
prefect gcl create github-api --limit 60 --slot-decay-per-second 0.016
```

これで、コード内でこのグローバル同時実行制限を使用して、API リクエストのレートを制限できるようになりました。

```python
from typing import Any
from prefect import task
from prefect.concurrency.sync import rate_limit

@task
def fetch_stats(github_repo: str) -> dict[str, Any]:
    """Task 1: Fetch the statistics for a GitHub repo"""
    rate_limit("github-api")
    return httpx.get(f"https://api.github.com/repos/{github_repo}").json()
```

<Expandable title="full example">
同時実行制限の動作を確認するには、次のコードを実行します:

```python
from typing import Any

import httpx
from prefect import flow, task
from prefect.concurrency.sync import rate_limit

@task(retries=3)
def fetch_stats(github_repo: str) -> dict[str, Any]:
    """Task 1: Fetch the statistics for a GitHub repo"""
    rate_limit("github-api")
    return httpx.get(f"https://api.github.com/repos/{github_repo}").json()


@task
def get_stars(repo_stats: dict[str, Any]) -> int:
    """Task 2: Get the number of stars from GitHub repo statistics"""
    return repo_stats["stargazers_count"]


@flow(log_prints=True)
def show_stars(github_repos: list[str]) -> None:
    """Flow: Show number of GitHub repo stars"""

    # Task 1: Make HTTP requests concurrently
    stats_futures = fetch_stats.map(github_repos)

    # Task 2: Once each concurrent task completes, get the star counts
    stars = get_stars.map(stats_futures).result()

    # Show the results
    for repo, star_count in zip(github_repos, stars):
        print(f"{repo}: {star_count} stars")


# Run the flow
if __name__ == "__main__":
    show_stars([
        "PrefectHQ/prefect",
        "pydantic/pydantic",
        "huggingface/transformers"
    ])
```
</Expandable>

## タスクの結果をキャッシュする

効率性を高めるため、すでに実行済みのタスクをスキップできます。
例えば、特定のリポジトリのスター数を1日に2回以上取得したくない場合は、その結果を1日分キャッシュしておくことができます。

```python
from typing import Any
from datetime import timedelta

from prefect import task
from prefect.cache_policies import INPUTS

@task(cache_policy=INPUTS, cache_expiration=timedelta(days=1))
def fetch_stats(github_repo: str) -> dict[str, Any]:
    """Task 1: Fetch the statistics for a GitHub repo"""
    # ...
```

<Expandable title="full example">
キャッシュの動作を確認するには、次のコードを実行します:

```python
from typing import Any
from datetime import timedelta

import httpx
from prefect import flow, task
from prefect.cache_policies import INPUTS
from prefect.concurrency.sync import rate_limit

@task(
    retries=3,
    cache_policy=INPUTS,
    cache_expiration=timedelta(days=1)
)
def fetch_stats(github_repo: str) -> dict[str, Any]:
    """Task 1: Fetch the statistics for a GitHub repo"""
    rate_limit("github-api")
    return httpx.get(f"https://api.github.com/repos/{github_repo}").json()


@task
def get_stars(repo_stats: dict[str, Any]) -> int:
    """Task 2: Get the number of stars from GitHub repo statistics"""
    return repo_stats["stargazers_count"]


@flow(log_prints=True)
def show_stars(github_repos: list[str]) -> None:
    """Flow: Show number of GitHub repo stars"""

    # Task 1: Make HTTP requests concurrently
    stats_futures = fetch_stats.map(github_repos)

    # Task 2: Once each concurrent task completes, get the star counts
    stars = get_stars.map(stats_futures).result()

    # Show the results
    for repo, star_count in zip(github_repos, stars):
        print(f"{repo}: {star_count} stars")


# Run the flow
if __name__ == "__main__":
    show_stars([
        "PrefectHQ/prefect",
        "pydantic/pydantic",
        "huggingface/transformers"
    ])
```
</Expandable>

## 改善したフローを実行します

これらの改善をすべて適用した後のフローは次のようになります:

```python my_data_pipeline.py
from typing import Any
from datetime import timedelta

import httpx
from prefect import flow, task
from prefect.cache_policies import INPUTS
from prefect.concurrency.sync import rate_limit

@task(
    retries=3,
    cache_policy=INPUTS,
    cache_expiration=timedelta(days=1)
)
def fetch_stats(github_repo: str) -> dict[str, Any]:
    """Task 1: Fetch the statistics for a GitHub repo"""
    rate_limit("github-api")
    return httpx.get(f"https://api.github.com/repos/{github_repo}").json()


@task
def get_stars(repo_stats: dict[str, Any]) -> int:
    """Task 2: Get the number of stars from GitHub repo statistics"""
    return repo_stats["stargazers_count"]


@flow(log_prints=True)
def show_stars(github_repos: list[str]) -> None:
    """Flow: Show number of GitHub repo stars"""

    # Task 1: Make HTTP requests concurrently
    stats_futures = fetch_stats.map(github_repos)

    # Task 2: Once each concurrent task completes, get the star counts
    stars = get_stars.map(stats_futures).result()

    # Show the results
    for repo, star_count in zip(github_repos, stars):
        print(f"{repo}: {star_count} stars")


# Run the flow
if __name__ == "__main__":
    show_stars([
        "PrefectHQ/prefect",
        "pydantic/pydantic",
        "huggingface/transformers"
    ])
```

フローを 2 回実行します。1 回目はタスクを実行して結果をキャッシュし、もう 1 回目はキャッシュから結果を取得します。

```bash
# Run the tasks and cache the results
python my_data_pipeline.py

# Run again (notice the cached results)
python my_data_pipeline.py
```

2 回目のフロー実行からのターミナル出力は次のようになります:

```bash
20:03:04.398 | INFO | prefect.engine - Created flow run 'laughing-nightingale' for flow 'show-stars'
20:03:05.146 | INFO | Task run 'fetch_stats-90f' - Finished in state Cached(type=COMPLETED)
20:03:05.149 | INFO | Task run 'fetch_stats-258' - Finished in state Cached(type=COMPLETED)
20:03:05.153 | INFO | Task run 'fetch_stats-924' - Finished in state Cached(type=COMPLETED)
20:03:05.159 | INFO | Task run 'get_stars-3a9' - Finished in state Completed()
20:03:05.159 | INFO | Task run 'get_stars-ed3' - Finished in state Completed()
20:03:05.161 | INFO | Task run 'get_stars-39c' - Finished in state Completed()
20:03:05.162 | INFO | Flow run 'laughing-nightingale' - PrefectHQ/prefect: 17756 stars
20:03:05.163 | INFO | Flow run 'laughing-nightingale' - pydantic/pydantic: 21613 stars
20:03:05.163 | INFO | Flow run 'laughing-nightingale' - huggingface/transformers: 136166 stars
20:03:05.339 | INFO | Flow run 'laughing-nightingale' - Finished in state Completed()
```

## 次のステップ

このチュートリアルでは、以下の手法を用いた、回復力とパフォーマンスに優れたデータパイプラインを構築しました。

- [再試行](/v3/develop/write-tasks#retries)：一時的なエラーへの対処
- [同時実行](/v3/develop/task-runners)：低速タスクの高速化
- [同時実行制限](/v3/develop/global-concurrency-limits)：APIのレート制限への到達回避
- [キャッシュ](/v3/develop/task-caching)：繰り返しタスクのスキップ

次に、[データ依存関係の処理と大量データの取り込み](/v3/tutorials/scraping)の方法を学びます。
エラー処理、ページ区切り、ネストされたフローを使用して、GitHubからデータをスクレイピングします。

<Tip>
サポートが必要ですか? Prefect 製品アドボケートとの[ミーティングを予約](https://calendly.com/prefect-experts/prefect-product-advocates?utm_campaign=prefect_docs_cloud&utm_content=prefect_docs&utm_medium=docs&utm_source=docs)して、ご質問にお答えします。
</Tip>