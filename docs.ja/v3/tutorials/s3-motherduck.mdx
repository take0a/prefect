---
title: クラウドストレージおよびデータウェアハウスとの統合
description: S3 と MotherDuck にデータを保存する ETL パイプラインを構築します。
---
import { Arcade } from '/snippets/arcade.mdx'

このチュートリアルでは、メジャーリーグベースボール (MLB) のゲームデータを S3 と MotherDuck で処理して保存する ETL パイプラインを作成します。

## 前提条件

* 無料の [Prefect Cloud アカウント](https://app.prefect.cloud/)
* 無料の [MotherDuck アカウント](https://motherduck.com/)
* [AWS アカウント](https://aws.amazon.com/free/)

### 環境設定

チュートリアルのコードサンプルは、[dev-day-zoom-out リポジトリ](https://github.com/PrefectHQ/dev-day-zoom-out) にあります。

まず、以下のコマンドを実行して `dev-day-zoom-out` リポジトリをクローンします:

```bash
git clone https://github.com/PrefectHQ/dev-day-zoom-out.git
```

リポジトリのルートにある `README.md` ファイルに従って環境を設定してください。
環境を設定したら、このチュートリアルのすべてのコードが配置されているディレクトリに移動します。

```bash
cd dev-day-zoom-out/track_1_build_workflows/session_2_resilent_workflows/1_starting_flow
```

既存の例を参考にして作業していただいても結構ですが、最初からやり直したい場合は、チュートリアルを最初から最後まで実行して、独自のパイプラインを構築してください。

## S3 と MotherDuck のリソースを設定する

パイプラインを構築する前に、AWS、MotherDuck、Prefect Cloud で必要なリソースを設定する必要があります。
AWS の経験豊富なユーザーで、IAM ユーザーと S3 バケットを既に用意している場合は、[Prefect AWS 認証情報ブロックの作成](#create-a-prefect-aws-credentials-block) セクションに進んでください。

### AWS リソースと Prefect ブロックを作成する

以下の手順に従って、ワークフローに必要な AWS リソースと Prefect ブロックを設定してください。
[AWS コンソール](https://aws.amazon.com/console/) にサインインし、AWS IAM ユーザーと AWS S3 バケットを作成します。
[Prefect Cloud](https://app.prefect.cloud/) にサインインし、Prefect AWS 認証情報ブロックと S3 バケットブロックを作成します。

#### AWS IAM ユーザーの設定

[AWS ドキュメント](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html) に従って、`AmazonS3FullAccess` 権限を持つ IAM ユーザーを作成してください。

<Tip>
**アクセス キー ID** と **シークレット アクセス キー** を保存します。これらは、Prefect AWS 認証情報ブロックを作成するために必要になります。
</Tip>

#### AWS S3 バケットを作成する

[AWS ドキュメント](https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html) に従って、AWS S3 バケットを作成します。

<Tip>
バケット名を保存します。Prefect S3 バケット ブロックを作成するときに必要になります。
</Tip>

#### Prefect AWS 認証情報ブロックを作成します

IAM ユーザーのアクセスキー ID とシークレットアクセスキーは、Prefect AWS 認証情報ブロックに保存できます。
この Prefect ブロックにより、フローとタスクが AWS リソースに接続できるようになります。
このブロックは、Python SDK または Prefect Cloud UI を使用して作成できます。

<Tabs>
  <Tab title="Python SDK method">
    プロジェクト ディレクトリに新しい Python ファイルを作成し、次のコードを追加します:

{/* pmd-metadata: notest */}
```python
from prefect_aws import AwsCredentials


AwsCredentials(
    aws_access_key_id="PLACEHOLDER",  # Replace this with your access key id.
    aws_secret_access_key="PLACEHOLDER",  # Replace this with your secret access key.
    region_name="us-east-2"  # Replace this with your region.
).save("BLOCK-NAME-PLACEHOLDER")  # Replace this with a descriptive block name.

```
<Tip>
スクリプトを実行する前に、環境に `prefect-aws` がインストールされていることを確認してください。
</Tip>

  </Tab>
  <Tab title="Prefect Cloud UI method">
    <Arcade src={"https://demo.arcade.software/hNTXKvOyX8d36VBRX7rB"} title={"Create an AWS Credentials Block"} />


  </Tab>
</Tabs>



#### Prefect S3 バケットブロックを作成する

S3 バケットブロックは、S3 バケットの設定を保存するために使用されます。
フローとタスクは、このブロックを使用して S3 バケットに接続します。
AWS 認証情報ブロックと同様に、このブロックも Python SDK または Prefect Cloud UI を使用して作成できます。


<Tabs>
  <Tab title="Python SDK method">
    プロジェクト ディレクトリに新しい Python ファイルを作成し、次のコードを追加します:

{/* pmd-metadata: notest */}
```python
from prefect_aws import S3Bucket, AwsCredentials

aws_credentials = AwsCredentials.load("NAME-OF-YOUR-AWS-CREDENTIALS-BLOCK") #Replace this with your AWS credentials block name.

S3Bucket(
    bucket_name="YOUR-S3-BUCKET-NAME", #Replace this with your S3 bucket name.
    credentials=aws_credentials #Replace this with your AWS credentials block name.
).save("BLOCK-NAME-PLACEHOLDER") #Replace this with a descriptive block name.
```
<Tip>
重要: スクリプトを実行する前に、環境に `prefect-aws` がインストールされていることを確認してください。
</Tip>

  </Tab>
  <Tab title="Prefect Cloud UI method">
    <Arcade src={"https://demo.arcade.software/7NAcl1fsMxLsvxI3Wsv0"} title={"Create an S3 Bucket block"} />

  </Tab>
</Tabs>

### MotherDuck アクセストークンとシークレットブロックを作成する

このセクションでは、MotherDuck アクセストークンと、トークンを保存するための Prefect シークレットブロックを作成する手順を説明します。

#### MotherDuck アクセストークンを作成する

[MotherDuck のドキュメント](https://motherduck.com/docs/key-tasks/authenticating-and-connecting-to-motherduck/authenticating-to-motherduck/#authentication-using-an-access-token) に従ってアクセストークンを作成してください。

<Tip>
トークンを保存します。これは、Prefect Secret ブロックを作成するときに必要になります。
</Tip>

#### Prefect シークレットブロックを作成する

Prefect シークレットブロックは、MotherDuck アクセストークンなどの機密情報を保存するために使用されます。
このシークレットブロックにより、フローとタスクが MotherDuck データベースに接続できるようになります。
シークレットブロックは、Python SDK または Prefect Cloud UI を使用して作成できます。

<Tabs>
  <Tab title="Python SDK method">
    プロジェクト ディレクトリに新しい Python ファイルを作成し、次のコードを追加します:

{/* pmd-metadata: notest */}
```python
from prefect_secrets import Secret

Secret(
    value="YOUR-MOTHERDUCK-TOKEN", #Replace this with your motherduck access token.
    name="motherduck-access-token" #Replace this with a name that will help you identify the secret.
).save("BLOCK-NAME-PLACEHOLDER") #Replace this with a descriptive block name.
```

  </Tab>
  <Tab title="Prefect Cloud UI method">
    <Arcade src={"https://demo.arcade.software/lkmbR08gd2qhoCqPtLGs"} title={"Create a Secret Block"} />
    
  </Tab>
</Tabs>

## プロジェクトをセットアップする

Prefectブロックの準備ができたので、新しいプロジェクトディレクトリを作成し、パイプラインを構築しましょう。

### 新しいプロジェクトディレクトリを作成します

プロジェクトファイルを保存するための新しいディレクトリをコンピューター上に作成します。
ディレクトリのルートに、ゲームデータの前処理と後処理、そしてPrefectフロー用のPythonファイルを保存する3つのサブフォルダを作成します。

```
.
└── mlb-data-project/
    ├── mlb_flow.py
    ├── boxscore_parquet
    ├── raw_data
    └── boxscore_analysis
```

次のコマンドを実行して、プロジェクト ディレクトリとファイルを作成します:

```bash
mkdir mlb-data-project
cd mlb-data-project
mkdir boxscore_parquet
mkdir raw_data
mkdir boxscore_analysis
touch mlb_flow.py
```

### MLB パイプラインの構築

プロジェクトディレクトリの設定が完了したら、いよいよ MLB ETL パイプラインの構築という楽しい作業に取り掛かります。

#### ゲームデータを取得する

まず、必要なパッケージをインポートし、[statsapi パッケージ](https://github.com/toddrob99/MLB-StatsAPI/tree/master?tab=readme-ov-file) を使用してゲームデータを取得する3つのタスクを作成します。
また、これらのタスクを呼び出し、タスク間の依存関係を定義するフローも作成します。

<Expandable title="data ingestion tasks">
{/* pmd-metadata: notest */}
```python
from prefect import flow, task, runtime
from prefect.artifacts import create_markdown_artifact
from prefect_aws.s3 import S3Bucket
from prefect.blocks.system import Secret
from datetime import datetime
import statsapi
import json
import pandas as pd
import duckdb

@task
def get_recent_games(team_name, start_date, end_date):
    '''This task will fetch the schedule for the provided team and date range and return the game ids.'''
    team = statsapi.lookup_team(team_name)
    schedule = statsapi.schedule(team=team[0]["id"], start_date=start_date, end_date=end_date)
    for game in schedule:
        print(game['game_id'])
    return [game['game_id'] for game in schedule]


@task
def fetch_single_game_boxscore(game_id, start_date, end_date, team_name):
    '''This task will fetch the boxscore for a single game and return the game data.'''
    boxscore = statsapi.boxscore_data(game_id)
    
    # Extract the relevant data.
    home_score = boxscore['home']['teamStats']['batting']['runs']
    away_score = boxscore['away']['teamStats']['batting']['runs']
    home_team = boxscore['teamInfo']['home']['teamName']
    away_team = boxscore['teamInfo']['away']['teamName']
    time_value = next(item['value'] for item in boxscore['gameBoxInfo'] if item['label'] == 'T')
    
    #Create a dictionary with the game data.
    game_data = {
        'search_start_date': start_date,
        'search_end_date': end_date,
        'chosen_team_name': team_name,
        'game_id': game_id,
        'home_team': home_team,
        'away_team': away_team,
        'home_score': home_score,
        'away_score': away_score,
        'score_differential': abs(home_score - away_score),
        'game_time': time_value,
    }
    
    print(game_data)
    return game_data

@flow
def mlb_flow(team_name, start_date, end_date):
    # Get recent games.
    game_ids = get_recent_games(team_name, start_date, end_date)
    
    # Fetch boxscore for each game.
    game_data = [fetch_single_game_boxscore(game_id, start_date, end_date, team_name) for game_id in game_ids]
    
    #Define file path for raw data.
    today = datetime.now().strftime("%Y-%m-%d")  # This uses the current date in the format YYYY-MM-DD.
    flow_run_name = runtime.flow_run.name
    raw_file_path = f"./raw_data/{today}-{team_name}-{flow_run_name}-boxscore.json"
    
    # Save raw data to a local folder.
    save_raw_data_to_file(game_data, raw_file_path)

if __name__ == "__main__":
    mlb_flow("marlins", '06/01/2024', '06/30/2024')

```
</Expandable>

このフローを実行すると、プロジェクト ディレクトリの `raw_data` フォルダーに保存された生データが表示されます。

#### S3 へのデータのロード

次の 2 つのタスクでは、生データファイルを S3 にアップロードし、さらに処理するために S3 に戻します。
これらの新しいタスクもフロー関数に追加する必要があります。
次のコードを `mlb_flow.py` ファイルにコピーして貼り付けます。

<Expandable title="S3 tasks">
{/* pmd-metadata: notest */}
```python

@task
def upload_raw_data_to_s3(file_path):
    '''This task will upload the raw data to s3.'''
    
    s3_bucket = S3Bucket.load("mlb-raw-data")  # Replace this with your S3 bucket block name.
    s3_bucket_path = s3_bucket.upload_from_path(file_path)
    
    print(s3_bucket_path)
    return s3_bucket_path
    

@task
def download_raw_data_from_s3(s3_file_path):
    '''Download the raw data from s3.'''
    
    s3_bucket = S3Bucket.load("mlb-raw-data")  # Replace this with your S3 bucket block name.
    local_file_path = f"./boxscore_analysis/{s3_file_path}"
    s3_bucket.download_object_to_path(s3_file_path, local_file_path)
    
    return local_file_path

```
</Expandable>

新しいタスクを含めるように `mlb_flow` 関数を更新します。

<Expandable title="updated flow function">
{/* pmd-metadata: notest */}
```python
@flow
def mlb_flow(team_name, start_date, end_date):
    # Get recent games.
    game_ids = get_recent_games(team_name, start_date, end_date)
    
    # Fetch boxscore for each game.
    game_data = [fetch_single_game_boxscore(game_id, start_date, end_date, team_name) for game_id in game_ids]
    
    # Define file path for raw data.
    today = datetime.now().strftime("%Y-%m-%d")  # This uses the current date in the format YYYY-MM-DD.
    flow_run_name = runtime.flow_run.name
    raw_file_path = f"./raw_data/{today}-{team_name}-{flow_run_name}-boxscore.json"
    
    # Save raw data to a local folder.
    save_raw_data_to_file(game_data, raw_file_path)
    
    # Upload raw data to s3.
    s3_file_path = upload_raw_data_to_s3(raw_file_path)
    
    # Download raw data from s3.
    raw_data = download_raw_data_from_s3(s3_file_path)

if __name__ == "__main__":
    mlb_flow("marlins", '06/01/2024', '06/30/2024')


```
</Expandable>

フロー関数を更新したら、スクリプトを実行して、データ ファイルが S3 バケットに到着することを確認できます。

#### データ処理とMotherDuckタスクの定義

生データをクリーンアップし、基本的な統計を実行し、結果をMotherDuckデータベースに保存する一連の関数を作成します。
以下のコードをコピーして、`mlb_flow.py`ファイルに貼り付けます。

<Expandable title="data processing and MotherDuck tasks">
{/* pmd-metadata: notest */}
```python
@task
def clean_time_value(data_file_path):
    '''This task will clean the time value.'''
    
    try:
        with open(data_file_path, 'r') as f:
            game_data_list = json.load(f)
    except FileNotFoundError:
        raise ValueError(f"File not found: {data_file_path}") 
    except json.JSONDecodeError:
        raise ValueError(f"Invalid JSON file: {data_file_path}")
    
    # Process each game in the list.
    for game_data in game_data_list:
        # Remove any extra text like '(1:16 delay)'
        if '(' in game_data['game_time']:
            game_data['game_time'] = game_data['game_time'].split('(')[0]
        
        # Remove any non-digit, non-colon characters.
        game_data['game_time'] = ''.join(char for char in game_data['game_time'] if char.isdigit() or char == ':')
        
        hours, minutes = map(int, game_data['game_time'].split(':'))
        game_data['game_time_in_minutes'] = hours * 60 + minutes
    
    # Save the modified data back to the file
    with open(data_file_path, 'w') as f:
        json.dump(game_data_list, f, indent=4, sort_keys=True)
    
    return data_file_path
    
@task
def analyze_games(data_file_path):
    '''This task will analyze the game data and return the analysis.'''
    
    try:
        with open(data_file_path, 'r') as f:
            game_data = json.load(f)
    except FileNotFoundError:
        raise ValueError(f"File not found: {data_file_path}")
    except json.JSONDecodeError:
        raise ValueError(f"Invalid JSON file: {data_file_path}")
    
    # Convert to a DataFrame.
    df = pd.DataFrame(game_data)
    
    # Get the search parameters.
    start_date = df['search_start_date'].unique()[0]
    end_date = df['search_end_date'].unique()[0]
    team_name = df['chosen_team_name'].unique()[0]
    
    # Calculate average, median, max, and min differential
    avg_differential = float(df['score_differential'].mean())
    median_differential = float(df['score_differential'].median())
    max_differential = float(df['score_differential'].max())
    min_differential = float(df['score_differential'].min())

    
    # Calculate average, median, max, and min game time.
    avg_game_time = float(df['game_time_in_minutes'].mean())
    median_game_time = float(df['game_time_in_minutes'].median())
    max_game_time = float(df['game_time_in_minutes'].max())
    min_game_time = float(df['game_time_in_minutes'].min())
    
    # Calculate correlation between game time and score differential.
    correlation = float(df['game_time_in_minutes'].corr(df['score_differential']))
    
    game_analysis = {
        'search_start_date': start_date,
        'search_end_date': end_date,
        'chosen_team_name': team_name,
        'max_game_time': max_game_time,
        'min_game_time': min_game_time,
        'median_game_time': median_game_time,
        'average_game_time': avg_game_time,
        'max_differential': max_differential,
        'min_differential': min_differential,
        'median_differential': median_differential,
        'average_differential': avg_differential,
        'time_differential_correlation': correlation,
    }
    print(game_analysis)
    return game_analysis

@task
def save_analysis_to_file(game_analysis, file_name):
    '''This task will save the analysis to a file.'''
    df = pd.DataFrame([game_analysis])
    df.to_parquet(file_name)
    
    print(file_name)
    return file_name

@task
def load_parquet_to_duckdb(parquet_file_path, team_name):
    '''This task will load the parquet file to duckdb.'''
    
    #Connect to duckdb.
    secret_block = Secret.load("mother-duck-token")  # Replace this with your secret block name.
    # Access the stored secret. 
    duck_token = secret_block.get()
    duckdb_conn = duckdb.connect(f"md:?motherduck_token={duck_token}")
    
    # Create a table in duckdb.
    duckdb_conn.execute(f"""CREATE TABLE IF NOT EXISTS boxscore_analysis_{team_name} (
        search_start_date TEXT, 
        search_end_date TEXT, 
        chosen_team_name TEXT, 
        max_game_time FLOAT, 
        min_game_time FLOAT, 
        median_game_time FLOAT, 
        average_game_time FLOAT, 
        max_differential FLOAT, 
        min_differential FLOAT, 
        median_differential FLOAT, 
        average_differential FLOAT, 
        time_differential_correlation FLOAT)""")
    
    # Use the COPY command to load the Parquet file into MotherDuck.
    duckdb_conn.execute(f"COPY boxscore_analysis FROM '{parquet_file_path}' (FORMAT 'parquet')")

```
</Expandable>

新しいタスクを含めるように `mlb_flow` 関数を更新します。

<Expandable title="updated flow function">
{/* pmd-metadata: notest */}
```python
# Update the mlb_flow function to include the new tasks.    
@flow
def mlb_flow(team_name, start_date, end_date):
    # Get recent games.
    game_ids = get_recent_games(team_name, start_date, end_date)
    
    # Fetch boxscore for each game.
    game_data = [fetch_single_game_boxscore(game_id, start_date, end_date, team_name) for game_id in game_ids]
    
    #Define file path for raw data.
    today = datetime.now().strftime("%Y-%m-%d")  # This uses the current date in the format YYYY-MM-DD.
    flow_run_name = runtime.flow_run.name
    raw_file_path = f"./raw_data/{today}-{team_name}-{flow_run_name}-boxscore.json"
    
    # Save raw data to a local folder.
    save_raw_data_to_file(game_data, raw_file_path)
    
    # Upload raw data to s3.
    s3_file_path = upload_raw_data_to_s3(raw_file_path)
    
    # Download raw data from s3.
    raw_data = download_raw_data_from_s3(s3_file_path)
    
    # Clean the time value.
    clean_data = clean_time_value(raw_data)
    
    # Analyze the results.
    results = analyze_games(clean_data)
    
    # Save the results to a file.
    parquet_file_path = f"./boxscore_parquet/{today}-{team_name}-{flow_run_name}-game-analysis.parquet"
    save_analysis_to_file(results, parquet_file_path)
    
    # Load the results to duckdb
    load_parquet_to_duckdb(parquet_file_path, team_name)

if __name__ == "__main__":
    mlb_flow("marlins", '06/01/2024', '06/30/2024')

```
</Expandable>


フロー関数を更新したら、スクリプトを実行して、処理済みのデータがMotherDuckデータベースに到着するのを確認できます。

#### Markdownアーティファクトを追加する

最後のステップとして、フローの結果を可視化します。
Prefect Cloud UIで結果を表示するために、Markdownアーティファクトを作成するタスクを追加できます。
以下のコードを`mlb_flow.py`ファイルにコピー＆ペーストしてください。

<Expandable title="markdown Artifact task">
{/* pmd-metadata: notest */}
```python
@task
def game_analysis_artifact(game_analysis, game_data_path):
    '''This task will create an artifact with the game analysis.'''
    
    # First read the JSON data from the file.
    with open(game_data_path, 'r') as f:
        game_data = json.load(f)
    
    # Now create the DataFrame from the loaded data.
    df = pd.DataFrame(game_data)
    
    # Create the markdown report.
    markdown_report=f""" # Game Analysis Report
## Search Parameters
Search Start Date: {game_analysis['search_start_date']}
Search End Date: {game_analysis['search_end_date']}
Chosen Team Name: {game_analysis['chosen_team_name']}

## Summary Statistics
Max game time: {game_analysis['max_game_time']:.2f}
Min game time: {game_analysis['min_game_time']:.2f}
Median game time: {game_analysis['median_game_time']:.2f}
Average game time: {game_analysis['average_game_time']:.2f}
Max differential: {game_analysis['max_differential']:.2f}
Min differential: {game_analysis['min_differential']:.2f}
Median differential: {game_analysis['median_differential']:.2f}
Average differential: {game_analysis['average_differential']:.2f}
Correlation between game time and score differential: {game_analysis['time_differential_correlation']:.2f}

## Raw Data
{df.to_markdown(index=False)}

"""
    create_markdown_artifact(
        key="game-analysis",
        markdown=markdown_report,
        description="Game analysis report"
    )

```
</Expandable>

新しいタスクを含めるように `mlb_flow` 関数を更新します。

<Expandable title="updated flow function">
{/* pmd-metadata: notest */}
```python
@flow
def mlb_flow(team_name, start_date, end_date):
    # Get recent games.
    game_ids = get_recent_games(team_name, start_date, end_date)
    
    # Fetch boxscore for each game.
    game_data = [fetch_single_game_boxscore(game_id, start_date, end_date, team_name) for game_id in game_ids]
    
    #Define file path for raw data.
    today = datetime.now().strftime("%Y-%m-%d")  # This uses the current date in the format YYYY-MM-DD.
    flow_run_name = runtime.flow_run.name
    raw_file_path = f"./raw_data/{today}-{team_name}-{flow_run_name}-boxscore.json"
    
    # Save raw data to a local folder.
    save_raw_data_to_file(game_data, raw_file_path)
    
    # Upload raw data to s3.
    s3_file_path = upload_raw_data_to_s3(raw_file_path)
    
    #Download raw data from s3.
    raw_data = download_raw_data_from_s3(s3_file_path)
    
    # Clean the time value.
    clean_data = clean_time_value(raw_data)
    
    # Analyze the results.
    results = analyze_games(clean_data)
    
    # Save the results to a file.
    parquet_file_path = f"./boxscore_parquet/{today}-{team_name}-{flow_run_name}-game-analysis.parquet"
    save_analysis_to_file(results, parquet_file_path)
    
    # Load the results to duckdb.
    load_parquet_to_duckdb(parquet_file_path, team_name)
    
    # Save the results to an artifact.
    game_analysis_artifact(results, raw_data)
    
    
if __name__ == "__main__":
    mlb_flow("marlins", '06/01/2024', '06/30/2024')

```
</Expandable>

新しいコードを追加したら、更新されたフローを実行して、マークダウン成果物を確認します。

<Arcade src={"https://demo.arcade.software/LHzqGzfl1r3AF3Nbcwvp"} title={"MLB Flow Run with Artifact"} />

## 次のステップ

ETLパイプラインが完成したので、障害処理とデータ品質チェックを追加して、より耐障害性を高めることができます。
ノートパソコンを24時間365日稼働させておく必要がないように、サーバーレスインフラストラクチャへのデプロイも検討してみてください。
Prefectを使って実際にそれを実現する方法については、次のチュートリアルをご覧ください。[耐障害性の高いパイプラインをサーバーレスインフラストラクチャにデプロイする](/v3/tutorials/resilience-and-deployment)
