---
title: ビッグデータを処理する
description: ビッグデータで Prefect を使用するためのガイダンス。
---

このページでは、Python コードを編集することなく、大量のデータを処理する Prefect ワークフローの処理時間やメモリ使用量を削減する方法について説明します。
速度、メモリ、コンピューティング、ストレージの面で Python コードを最適化するには、次のようないくつかの方法があります。

1. `quote` を使用してタスクのイントロスペクションを削除し、コード実行時間を節約する。
1. ブロックを使用してタスクの結果を S3 などのクラウドストレージに書き込むことでメモリを節約する。
1. 結果を使用するのではなく、フロー内でデータをディスクに保存する。
1. タスクの結果をキャッシュして、時間とコンピューティングを節約する。
1. ディスクに書き込まれた結果を圧縮してスペースを節約する。
1. 並列処理可能な操作に [タスクランナー](/v3/develop/task-runners/) を使用して時間を節約する。

### タスクのイントロスペクションの削除

フローからタスクが呼び出されると、デフォルトでは各引数がPrefectによってイントロスペクションされます。
フローの実行速度を上げるには、引数を[`quote`](https://docs.prefect.io/latest/api-ref/prefect/utilities/annotations/#prefect.utilities.annotations.quote)で囲むことで、タスクのこの動作を無効にしてください。

以下は、ニューヨークのタクシーデータを抽出して変換する基本的な例です:

```python et_quote.py
from prefect import task, flow
from prefect.utilities.annotations import quote
import pandas as pd


@task
def extract(url: str):
    """Extract data"""
    df_raw = pd.read_parquet(url)
    print(df_raw.info())
    return df_raw


@task
def transform(df: pd.DataFrame):
    """Basic transformation"""
    df["tip_fraction"] = df["tip_amount"] / df["total_amount"]
    print(df.info())
    return df


@flow(log_prints=True)
def et(url: str):
    """ET pipeline"""
    df_raw = extract(url)
    df = transform(quote(df_raw))


if __name__ == "__main__":
    url = "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-09.parquet"
    et(url)
```

渡されるオブジェクトが辞書やデータフレームなどの大規模なコレクションで、各要素を参照する必要がある場合、イントロスペクションにかなりの時間がかかることがあります。

`quote` を使用すると、ラップされたオブジェクトのタスク依存関係の追跡が無効になりますが、実行時間が短縮されます。

### タスク結果をクラウドストレージに書き込む

デフォルトでは、タスク実行の結果は実行環境のメモリに保存されます。
この動作により、小さなデータの場合はフロー実行が高速になりますが、大きなデータの場合は問題が発生する可能性があります。
結果をディスクに書き込むことでメモリを節約できます。

本番環境では、AWS S3 などのクラウドプロバイダーのストレージに結果を書き込むことをお勧めします。
Prefect では、[prefect-aws](/integrations/prefect-aws) などの Prefect Cloud 統合ライブラリのストレージブロックを使用して設定情報を保存できます。
[blocks](/v3/develop/blocks/) の詳細については、こちらをご覧ください。

関連するライブラリをインストールし、サーバーにブロックタイプを登録して、ブロックを作成します。
次に、フロー内でブロックを参照します。

```python
...
from prefect_aws.s3 import S3Bucket

my_s3_block = S3Bucket.load("MY_BLOCK_NAME")

...
@task(result_storage=my_s3_block)

```

タスクの結果はメモリに保存されるのではなく、S3 に書き込まれます。

### フロー内でデータをディスクに保存する

{/*
<!-- vale off -->
*/}
To save memory and time with big data, you don't need to pass results between tasks.
Instead, you can write and read data to disk directly in your flow code.
Prefect has integration libraries for each of the major cloud providers.
{/*
<!-- vale on -->
*/}

各ライブラリには、クラウド オブジェクト ストレージとの間でデータの読み取りと書き込みを便利にするメソッドを備えたブロックが含まれています。


### タスク結果をキャッシュする

キャッシュを使用すると、タスクの不要な再実行を回避できるため、時間とコンピューティングリソースを節約できます。
キャッシュにはタスク結果の永続性が必要であることに注意してください。
[キャッシュ](/v3/develop/task-caching/) の詳細については、こちらをご覧ください。

### ディスクに書き込まれる結果を圧縮する

Prefect のタスク結果の永続化を使用している場合は、結果を圧縮してディスク容量を節約できます。
結果の型を `compressed/` プレフィックスを付けて指定します:

```python
@task(result_serializer="compressed/json")
```

圧縮にはデータの圧縮と解凍に時間がかかることに注意してください。

### 並列処理にはタスクランナーを使用する

Prefect のタスクランナーでは、Dask および Ray Python ライブラリを使用して、複数のマシンに分散したタスクを並列実行できます。
これにより、大規模なデータ構造を処理する際の時間と計算コストを節約できます。
詳細については、[Dask および Ray タスクランナーの使用ガイド](/v3/develop/task-runners/) をご覧ください。
